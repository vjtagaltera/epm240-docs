

tensorflow tutorial samples

toc
    example 1
    example 2
    example 2 results
    example 3 lenet-5 and result

=====================================================================
example 1

$ cat tut-tf-1.py
#!/usr/bin/env python3
#
# https://www.tensorflow.org/tutorials/quickstart/beginner
# ubuntu bionic: runs on tensorflow 2.0.2. 2.1.0 failed loading libcuda. python 3.6
# windows10: tensorflow 2.2.0. failed cudart64_101.dll is just a warning. python 3.8.2.

import os
import tensorflow as tf

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test/255.0

print(" model ... ")
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(128,activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

print(" predictions ... ")
predictions = model(x_train[:1]).numpy()
print(" predictions result ")
print("   %s" % str(predictions))

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss = loss_fn(y_train[:1], predictions).numpy()
print(" loss %.3f " % loss)

model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
model.fit(x_train, y_train, epochs=25)

print(" evaluate ... ")
evaluated = model.evaluate(x_test, y_test, verbose=2)
print(" evaluate result ")
print("   %s" % str(evaluated))

prob_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

test_results = prob_model(x_test[:5])
print(" probability result ")
print("   %s" % str(test_results))

#print(" softmax ... ")
#result = tf.nn.softmax(predictions).numpy()
#print(" softmax result ")
#print("   %s" % str(result))

=====================================================================
example 2

updated file: tut-tf-1-weights-save-2.py

$ cat tut-tf-1-weights-save.py
#!/usr/bin/env python3
#
# https://www.tensorflow.org/tutorials/quickstart/beginner
# ubuntu bionic: runs on tensorflow 2.0.2. 2.1.0 failed loading libcuda. python 3.6
# windows10: tensorflow 2.2.0. failed cudart64_101.dll is just a warning. python 3.8.2.

import os
import tensorflow as tf
import time

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

mnist = tf.keras.datasets.mnist
tm01 = time.time()
(x_train, y_train), (x_test, y_test) = mnist.load_data()
tm02 = time.time()
x_train, x_test = x_train / 255.0, x_test/255.0
print(" load_data cost %.3f " % (tm02 - tm01))

print(" model ... ")
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(128,activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

#print(" predictions ... ")
#predictions = model(x_train[:1]).numpy()
#print(" predictions result ")
#print("   %s" % str(predictions))

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
#loss = loss_fn(y_train[:1], predictions).numpy()
#print(" loss %.3f " % loss)

model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
if False:
    model.fit(x_train, y_train, epochs=2)
    model.save_weights(checkpoint_path.format(epoch=2))
else:
    latest = tf.train.latest_checkpoint(checkpoint_dir)
    model.load_weights(latest)

print(" evaluate ... ")
tm11 = time.time()
evaluated = model.evaluate(x_test, y_test, verbose=2)
tm12 = time.time()
print(" evaluate result ")
print("   %s" % str(evaluated))
print(" evaluate cost %.3f " % (tm12 - tm11))

prob_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

def prof_test(the_end=None):
    if the_end is None: the_end = 5
    if the_end < 5: the_end = 5
    if the_end > 10000: the_end = 10000
    return prob_model(x_test[:the_end])
tm21 = time.time()
test_results = prof_test(5)
tm22 = time.time()
test_results = prof_test(500)
tm23 = time.time()
test_results = prof_test(10000)
tm24 = time.time()
print(" probability result ")
print("   len %s" % str( len(test_results)))
print(" probability cost %.6f %.6f %.6f " % (tm22 - tm21, tm23-tm22, tm24-tm23))
test_ref_results = [7, 2, 1, 0, 4, 1] # y_test first 6 elements]
def print_ref(test_results, test_ref_results):
    retv = ""
    for i in range(6):
        if i >= len(test_results) or i >= len(test_ref_results): break
        retv += " %.3f" % test_results[i][test_ref_results[i]]
    return retv
print(" probability ref comparison : %s " % print_ref(test_results, test_ref_results))

def prof_train():
    return prob_model(x_train[:60000])
tm33 = time.time()
test_results = prof_train()
tm34 = time.time()
print(" probability result ")
print("   len %s" % str( len(x_train) ))
print(" probability cost %.6f " % (tm34 - tm33))
test_ref_results = [5, 0, 4, 1, 9, 2] # y_train first 6 elements
print(" probability ref comparison : %s " % print_ref(test_results, test_ref_results))

print("")
print(" model build cost %.3f " % (tm11 - tm02))
print(" total cost       %.3f " % (tm34 - tm01))

print("")
print(" platform computations G per second %.3f" % (28.0*28.0*128*10*len(x_train)/(tm34-tm33)/1000.0/1000.0/1000.0))

#print(" softmax ... ")
#result = tf.nn.softmax(predictions).numpy()
#print(" softmax result ")
#print("   %s" % str(result))

=====================================================================
example 2 results

i7-6700hq 2.6g , win10 , tf 2.2.0 on python 3.8.2 , GOPS: 
442 450 288 458 456 445

  when the GOPS is 480, 10k sample cost is 20ms, 60k is 125ms.
  each sample cost about 2.1us. 

i7-7700hq 2.8g , ubuntu 18.04 , tf 2.0.2 on python 3.6.9 , or tf 2.2.0
224 221 223 225 223 225

i7-7700hq 2.8g , ubuntu 18.04 , tf 2.2.0 , phthon 3.8.0
235 235 235 234 233 235

nx , ubuntu 18.04 , tf 1.15.2+nv20.4 , python 3.6.9 
159 or 160. when using cuda it is much slower. 


updated -2.py file: 
               time    sample time    cpu us       GOPS
win10:  10k    .395                     39.5
        60k   3.582                     59.7       16.8
        10k    .452                     45.2
        60k   3.660                     61.0       16.4

nx:     10k    .571                     57.2
        60k   3.390                     56.5       17.671
        10k    .594                     59.4
        60k   3.702                     61.7       16.263

  w cuda 60k  4.128                     68.8       14.584

=====================================================================
example 3 lenet-5 and result

updated file: tut-tf-2-lenet5-1.py and -2-nx.py

https://github.com/NVIDIA/DIGITS/blob/master/docs/GettingStarted.md
dataset used by the nx tensorrt sample sampleMNIST 

$ diff -u tut-tf-1-weights-save.py tut-tf-2-lenet5-1.py
--- tut-tf-1-weights-save.py    2020-06-28 08:03:52.175928200 -0700
+++ tut-tf-2-lenet5-1.py        2020-06-30 15:43:35.395463700 -0700
@@ -7,6 +7,7 @@
 import os
 import tensorflow as tf
 import time
+import numpy as np

 os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

@@ -18,25 +19,58 @@
 print(" load_data cost %.3f " % (tm02 - tm01))

 print(" model ... ")
-model = tf.keras.models.Sequential([
-  tf.keras.layers.Flatten(input_shape=(28,28)),
-  tf.keras.layers.Dense(128,activation='relu'),
-  tf.keras.layers.Dropout(0.2),
-  tf.keras.layers.Dense(10)
-])
+if True:
+    #https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086
+    model = tf.keras.Sequential()
+    model.add(tf.keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))
+    model.add(tf.keras.layers.AveragePooling2D())
+    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
+    model.add(tf.keras.layers.AveragePooling2D())
+    model.add(tf.keras.layers.Flatten())
+    model.add(tf.keras.layers.Dense(units=120, activation='relu'))
+    model.add(tf.keras.layers.Dense(units=84, activation='relu'))
+    ##model.add(tf.keras.layers.Dense(units=10, activation='softmax'))
+    model.add(tf.keras.layers.Dense(units=10))
+else:
+    model = tf.keras.models.Sequential([
+      tf.keras.layers.Flatten(input_shape=(28,28)),
+      tf.keras.layers.Dense(128,activation='relu'),
+      tf.keras.layers.Dropout(0.2),
+      tf.keras.layers.Dense(10)
+    ])

 #print(" predictions ... ")
-#predictions = model(x_train[:1]).numpy()
+# https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros
+#x_test_predict = np.ndarray( (1, 28, 28) )
+#x_test_predict = np.zeros(x_test_predict.shape)
+#x_test_predict = np.expand_dims(x_test_predict, axis=3)
+#predictions = model(x_test_predict[:1]).numpy()
+#
+#x_test_x2 = np.zeros( (x_test.shape[0], 32, 32) )
+#for i in range(x_test.shape[0]):
+#    x_test_x2[i][2:30, 2:30] = x_test[i][:][:]
+#predictions = model(x_test_x2[:1]).numpy()
 #print(" predictions result ")
 #print("   %s" % str(predictions))

+# pre-process data to fit the model input requirements
+x_test_x2 = np.zeros( (x_test.shape[0], 32, 32) )
+for i in range(x_test.shape[0]):
+    x_test_x2[i][2:30, 2:30] = x_test[i][:][:]
+x_test = np.expand_dims(x_test_x2, axis=3)
+
+x_train_x2 = np.zeros( (x_train.shape[0], 32, 32) )
+for i in range(x_train.shape[0]):
+    x_train_x2[i][2:30, 2:30] = x_train[i][:][:]
+x_train = np.expand_dims(x_train_x2, axis=3)
+
 loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
 #loss = loss_fn(y_train[:1], predictions).numpy()
 #print(" loss %.3f " % loss)

 model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

-checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
+checkpoint_path = "training_3/cp-{epoch:04d}.ckpt"
 checkpoint_dir = os.path.dirname(checkpoint_path)
 if False:
     model.fit(x_train, y_train, epochs=2)


result i7-6700hq win10 tf2.2.0: 
  10k samples cost  0.580 seconds. each sample costs 58us. 
  60k samples cost  3.520 seconds. 

nx 10w tf 1.15.2.  x_test x_train init to np.float32: run the prediction() on model
       cost                   cpu    cuda
  10k  .113  ==> sample time 341us   108us
  60k  .270  ==> sample time 337us   104us

=====================================================================


